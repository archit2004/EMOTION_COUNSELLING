{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3457df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "# Example list to simulate live predictions\n",
    "emotion_buffer = []\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"emotion_logs\",\n",
    "    user=\"postgres\",\n",
    "    password=\"23BCT0011\",\n",
    "    host=\"localhost\",  # or IP address\n",
    "    port=\"5432\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Helper: Insert median emotion into DB\n",
    "def insert_emotion(emotion,probability=None):\n",
    "    current_time = datetime.now()\n",
    "    prob = None if probability is None else float(probability)\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO emotion_data (timestamp, emotion,probability) VALUES (%s, %s,%s)\",\n",
    "        (current_time, emotion,prob)\n",
    "    )\n",
    "    conn.commit()\n",
    "    print(f\"[{current_time}] Logged: {emotion} ,{prob:.2%})\")\n",
    "\n",
    "\n",
    "# Function to display all rows in the emotion_data table\n",
    "def show_all_emotions():\n",
    "    try:\n",
    "        conn.rollback()  # <-- resets failed transaction state if any\n",
    "        cursor.execute(\"SELECT * FROM emotion_data ORDER BY ID DESC LIMIT 10;\")\n",
    "        rows = cursor.fetchall()\n",
    "        print(\"\\n--- Last 10 Emotion Entries ---\")\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "        print(\"------------------------------\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to fetch data:\", e)\n",
    "\n",
    "\n",
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93aff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from spektral.layers import GINConv, GCNConv\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mediapipe.python.solutions.face_mesh_connections import FACEMESH_TESSELATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd37c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mediapipe_adjacency_matrix():\n",
    "    \"\"\"Create adjacency matrix from MediaPipe face mesh connections\"\"\"\n",
    "    # MediaPipe face mesh has 468 landmarks\n",
    "    n_nodes = 468\n",
    "    adj_matrix = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "    \n",
    "    # Add edges based on MediaPipe face mesh connections\n",
    "    for connection in FACEMESH_TESSELATION:\n",
    "        i, j = connection[0], connection[1]\n",
    "        if i < n_nodes and j < n_nodes:  # Ensure indices are valid\n",
    "            adj_matrix[i, j] = 1.0\n",
    "            adj_matrix[j, i] = 1.0  # Undirected graph\n",
    "    \n",
    "    # Add self-loops\n",
    "    np.fill_diagonal(adj_matrix, 1.0)\n",
    "    \n",
    "    return adj_matrix\n",
    "\n",
    "def normalize_mesh_points(mesh_points):\n",
    "    \"\"\"Normalize mesh points to unit scale\"\"\"\n",
    "    mesh_points = np.array(mesh_points, dtype=np.float32)\n",
    "    \n",
    "    # Center the points\n",
    "    center = np.mean(mesh_points, axis=0)\n",
    "    mesh_points = mesh_points - center\n",
    "    \n",
    "    # Scale to unit variance\n",
    "    scale = np.std(mesh_points)\n",
    "    if scale > 0:\n",
    "        mesh_points = mesh_points / scale\n",
    "    \n",
    "    return mesh_points\n",
    "\n",
    "def load_mesh_data(path_list, limit=1.0):\n",
    "    \"\"\"Load and preprocess mesh data from directories\"\"\"\n",
    "    all_meshes = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for emotion_idx, path in enumerate(path_list):\n",
    "        if not path.exists():\n",
    "            print(f\"Path does not exist: {path}\")\n",
    "            continue\n",
    "            \n",
    "        files = os.listdir(path)\n",
    "        num_files = len(files)\n",
    "        files_to_process = int(num_files * limit)\n",
    "        \n",
    "        emotion_meshes = []\n",
    "        processed = 0\n",
    "        \n",
    "        for file in files[:files_to_process]:\n",
    "            if not file.endswith('.json'):\n",
    "                continue\n",
    "                \n",
    "            file_path = path / file\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                # Ensure we have exactly 468 landmarks\n",
    "                if len(data) != 468:\n",
    "                    print(f\"Skipping {file}: expected 468 landmarks, got {len(data)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Normalize the mesh points\n",
    "                normalized_mesh = normalize_mesh_points(data)\n",
    "                emotion_meshes.append(normalized_mesh)\n",
    "                processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Loaded {processed} samples for emotion {emotion_idx}\")\n",
    "        \n",
    "        # Add to overall lists\n",
    "        all_meshes.extend(emotion_meshes)\n",
    "        all_labels.extend([emotion_idx] * len(emotion_meshes))\n",
    "    \n",
    "    return np.array(all_meshes, dtype=np.float32), np.array(all_labels, dtype=np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b8f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from spektral.layers import GINConv\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute adjacency and sparse tensor\n",
    "\n",
    "def get_mediapipe_adj_tensor():\n",
    "    adj = get_mediapipe_adjacency_matrix()  # your existing function\n",
    "    adj_sp = scipy.sparse.csr_matrix(adj)\n",
    "    return sp_matrix_to_sp_tensor(adj_sp)\n",
    "\n",
    "# Build function\n",
    "def build_gnn_face_emotion_model(\n",
    "    N: int,\n",
    "    F: int,\n",
    "    n_out: int,\n",
    "    l2_reg: float = 5e-4,\n",
    "    learning_rate: float = 1e-3\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds and compiles the GNN face emotion classification model.\n",
    "\n",
    "    Args:\n",
    "        N: Number of nodes (e.g., 468).\n",
    "        F: Feature dimensionality per node (e.g., 2 for x, y).\n",
    "        n_out: Number of emotion classes.\n",
    "        l2_reg: L2 regularization factor.\n",
    "        learning_rate: Learning rate for the Adam optimizer.\n",
    "\n",
    "    Returns:\n",
    "        A compiled tf.keras.Model instance.\n",
    "    \"\"\"\n",
    "    # Prepare adjacency as sparse tensor\n",
    "    adj_tensor = get_mediapipe_adj_tensor()\n",
    "\n",
    "    # Input for node features\n",
    "    X_in = Input(shape=(N, F), name='node_features')\n",
    "\n",
    "    # First GNN block\n",
    "    x = GINConv(64, activation='relu', kernel_regularizer=l2(l2_reg))([X_in, adj_tensor])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Second GNN block\n",
    "    x = GINConv(32, activation='relu', kernel_regularizer=l2(l2_reg))([x, adj_tensor])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Global pooling (flatten)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Dense layers\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output\n",
    "    output = Dense(n_out, activation='softmax', name='emotion_output')(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=X_in, outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# model = build_gnn_face_emotion_model(N=468, F=2, n_out=6)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5105fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\OneDrive - vit.ac.in\\Documents\\Projects\\Emotion_counselling\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\OneDrive - vit.ac.in\\Documents\\Projects\\Emotion_counselling\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# (Re)define your `build_model()` exactly as you did originally,\n",
    "# including the `get_mediapipe_adjacency_matrix() + sp_matrix_to_sp_tensor` call.\n",
    "model = build_gnn_face_emotion_model(N=468, F=2, n_out=6)  \n",
    "model.load_weights(\"gnn_face_emotion2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55319fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ad642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained GNN model...\n",
      "❌ Error loading model: No module named 'model_utils'\n",
      "Make sure 'emotion_gnn_model.h5' exists and was trained with the GNN code.\n",
      "Creating adjacency matrix...\n",
      "Adjacency matrix shape: (468, 468)\n",
      "Number of edges: 1556\n",
      "Starting webcam...\n",
      "Press 'q' to quit, 's' to save current frame, 'r' to reset prediction history\n",
      "[2025-08-08 16:09:22.465361] Logged: Neutral ,22.17%)\n",
      "[2025-08-08 16:09:32.514823] Logged: Angry ,29.24%)\n",
      "[2025-08-08 16:09:42.536521] Logged: Neutral ,30.29%)\n",
      "[2025-08-08 16:09:52.566692] Logged: Angry ,21.03%)\n",
      "[2025-08-08 16:10:02.670311] Logged: Neutral ,27.49%)\n",
      "[2025-08-08 16:10:12.736046] Logged: Angry ,29.35%)\n",
      "[2025-08-08 16:10:22.827393] Logged: Angry ,14.06%)\n",
      "[2025-08-08 16:10:32.875387] Logged: Neutral ,37.65%)\n",
      "[2025-08-08 16:10:43.015488] Logged: Sad ,51.04%)\n",
      "[2025-08-08 16:10:53.135714] Logged: Neutral ,26.95%)\n",
      "[2025-08-08 16:11:03.163418] Logged: Angry ,30.09%)\n",
      "[2025-08-08 16:11:13.240987] Logged: Angry ,23.74%)\n",
      "[2025-08-08 16:11:23.300197] Logged: Happy ,7.86%)\n",
      "[2025-08-08 16:11:33.370795] Logged: Angry ,10.58%)\n",
      "[2025-08-08 16:11:43.403923] Logged: Angry ,27.77%)\n",
      "Shutting down...\n",
      "✅ Application closed successfully.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from spektral.layers import GINConv\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "import scipy.sparse\n",
    "from mediapipe.python.solutions.face_mesh_connections import FACEMESH_TESSELATION\n",
    "import time\n",
    "\n",
    "def get_mediapipe_adjacency_matrix():\n",
    "    \"\"\"Create adjacency matrix from MediaPipe face mesh connections\"\"\"\n",
    "    # MediaPipe face mesh has 468 landmarks (without iris when refine_landmarks=False)\n",
    "    n_nodes = 468\n",
    "    adj_matrix = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "    \n",
    "    # Add edges based on MediaPipe face mesh connections\n",
    "    for connection in FACEMESH_TESSELATION:\n",
    "        i, j = connection[0], connection[1]\n",
    "        if i < n_nodes and j < n_nodes:  # Ensure indices are valid for 468 landmarks\n",
    "            adj_matrix[i, j] = 1.0\n",
    "            adj_matrix[j, i] = 1.0  # Undirected graph\n",
    "    \n",
    "    # Add self-loops\n",
    "    np.fill_diagonal(adj_matrix, 1.0)\n",
    "    \n",
    "    return adj_matrix\n",
    "\n",
    "# --- Constants and Initializations ---\n",
    "# Define the emotion labels in the same order as your training data\n",
    "EMOTIONS = [\"Angry\", \"Disgusted\", \"Happy\", \"Neutral\", \"Sad\", \"Surprised\"]\n",
    "\n",
    "# Emotion colors for display (BGR format for OpenCV)\n",
    "EMOTION_COLORS = {\n",
    "    \"Angry\": (0, 0, 255),      # Red\n",
    "    \"Disgusted\": (0, 128, 0),  # Green\n",
    "    \"Happy\": (0, 255, 255),    # Yellow\n",
    "    \"Neutral\": (128, 128, 128), # Gray\n",
    "    \"Sad\": (255, 0, 0),        # Blue\n",
    "    \"Surprised\": (0, 165, 255) # Orange\n",
    "}\n",
    "\n",
    "# Load the trained model with improved method\n",
    "try:\n",
    "    print(\"Loading trained GNN model...\")\n",
    "    from model_utils import load_model_with_validation\n",
    "    model, metadata, adj_tensor_loaded = load_model_with_validation(\"emotion_gnn_model.h5\")\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"❌ Could not load model. Make sure you've trained the model first.\")\n",
    "        exit()\n",
    "        \n",
    "    print(\"✅ Model loaded successfully.\")\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Make sure 'emotion_gnn_model.h5' exists and was trained with the GNN code.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1, \n",
    "    refine_landmarks=False,  # Use 468 landmarks (no iris)\n",
    "    min_detection_confidence=0.7, \n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Create adjacency matrix for the model\n",
    "print(\"Creating adjacency matrix...\")\n",
    "adj_matrix = get_mediapipe_adjacency_matrix()\n",
    "adj_sparse = scipy.sparse.csr_matrix(adj_matrix)\n",
    "adj_tensor = sp_matrix_to_sp_tensor(adj_sparse)\n",
    "\n",
    "print(f\"Adjacency matrix shape: {adj_matrix.shape}\")\n",
    "print(f\"Number of edges: {np.sum(adj_matrix > 0) // 2}\")\n",
    "\n",
    "# Prediction smoothing\n",
    "prediction_history = []\n",
    "history_size = 5  # Number of frames to average for smoothing\n",
    "\n",
    "def normalize_mesh_points(mesh_points):\n",
    "    \"\"\"Normalize mesh points to match training data preprocessing\"\"\"\n",
    "    mesh_points = np.array(mesh_points, dtype=np.float32)\n",
    "    \n",
    "    # Center the points\n",
    "    center = np.mean(mesh_points, axis=0)\n",
    "    mesh_points = mesh_points - center\n",
    "    \n",
    "    # Scale to unit variance\n",
    "    scale = np.std(mesh_points)\n",
    "    if scale > 0:\n",
    "        mesh_points = mesh_points / scale\n",
    "    \n",
    "    return mesh_points\n",
    "\n",
    "def draw_prediction_bars(frame, predictions, current_emotion):\n",
    "    \"\"\"Draw probability bars for all emotions\"\"\"\n",
    "    bar_width = 200\n",
    "    bar_height = 25\n",
    "    start_x = 10\n",
    "    start_y = 80\n",
    "    \n",
    "    for i, (emotion, prob) in enumerate(zip(EMOTIONS, predictions[0])):\n",
    "        y_pos = start_y + i * (bar_height + 5)\n",
    "        \n",
    "        # Background bar\n",
    "        cv2.rectangle(frame, (start_x, y_pos), (start_x + bar_width, y_pos + bar_height), \n",
    "                     (50, 50, 50), -1)\n",
    "        \n",
    "        # Probability bar\n",
    "        fill_width = int(bar_width * prob)\n",
    "        color = EMOTION_COLORS[emotion] if emotion == current_emotion else (100, 100, 100)\n",
    "        cv2.rectangle(frame, (start_x, y_pos), (start_x + fill_width, y_pos + bar_height), \n",
    "                     color, -1)\n",
    "        \n",
    "        # Text label\n",
    "        cv2.putText(frame, f\"{emotion}: {prob:.1%}\", (start_x + bar_width + 10, y_pos + 18), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "# --- Main Webcam Loop ---\n",
    "print(\"Starting webcam...\")\n",
    "print(\"Press 'q' to quit, 's' to save current frame, 'r' to reset prediction history\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set camera properties for better performance\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "frame_count = 0\n",
    "fps_start_time = time.time()\n",
    "fps = 0\n",
    "start_time_window = time.time()  # ← add this\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    H, W, _ = frame.shape\n",
    "    \n",
    "    # Calculate FPS\n",
    "    if frame_count % 30 == 0:\n",
    "        fps_end_time = time.time()\n",
    "        fps = 30 / (fps_end_time - fps_start_time)\n",
    "        fps_start_time = fps_end_time\n",
    "\n",
    "    # Flip frame horizontally for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the frame with MediaPipe\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        # Use the landmarks of the first detected face\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        \n",
    "        # --- Extract and preprocess landmarks for the model ---\n",
    "        # 1. Extract X, Y coordinates\n",
    "        mesh_points = np.array(\n",
    "            [[p.x * W, p.y * H] for p in face_landmarks.landmark],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # print(f\"Number of landmarks detected: {len(mesh_points)}\")  # Should be 468\n",
    "        \n",
    "        # 2. Normalize the landmarks (same as training)\n",
    "        normalized_points = normalize_mesh_points(mesh_points)\n",
    "        \n",
    "        # 3. Reshape for the model input (add batch dimension)\n",
    "        model_input = np.expand_dims(normalized_points, axis=0)\n",
    "        try:\n",
    "            prediction = model.predict(model_input, verbose=0)\n",
    "            emotion_index = np.argmax(prediction[0])\n",
    "    #         emotion_buffer.append(EMOTIONS[emotion_index])\n",
    "\n",
    "    # # Every 10 seconds: calculate median and log\n",
    "    #         if time.time() - start_time_window >= 10:\n",
    "    #             if emotion_buffer:\n",
    "    #                 median_emotion = statistics.mode(emotion_buffer)\n",
    "    #                 insert_emotion(median_emotion)\n",
    "    #                 emotion_buffer.clear()\n",
    "    #             start_time_window = time.time()\n",
    "            # Add to prediction history for smoothing\n",
    "            prediction_history.append(prediction[0])\n",
    "            if len(prediction_history) > history_size:\n",
    "                prediction_history.pop(0)\n",
    "            \n",
    "            # Average predictions for smoother results\n",
    "            smoothed_prediction = np.mean(prediction_history, axis=0)\n",
    "            emotion_index = np.argmax(smoothed_prediction)\n",
    "            emotion_label = EMOTIONS[emotion_index]\n",
    "            confidence = smoothed_prediction[emotion_index]\n",
    "\n",
    "            emotion_buffer.append(emotion_label)\n",
    "\n",
    "            if time.time() - start_time_window >= 10:\n",
    "                if emotion_buffer:\n",
    "                    median_emotion = statistics.mode(emotion_buffer)\n",
    "                    # Extract the confidence for that label from your last smoothed_prediction:\n",
    "                    median_confidence = smoothed_prediction[EMOTIONS.index(median_emotion)]\n",
    "                    insert_emotion(median_emotion, median_confidence)\n",
    "                    emotion_buffer.clear()\n",
    "                start_time_window = time.time()\n",
    "\n",
    "            \n",
    "            # --- Display the result on the frame ---\n",
    "            # Draw face bounding box\n",
    "            x_min = int(np.min(mesh_points[:, 0])) - 20\n",
    "            y_min = int(np.min(mesh_points[:, 1])) - 20\n",
    "            x_max = int(np.max(mesh_points[:, 0])) + 20\n",
    "            y_max = int(np.max(mesh_points[:, 1])) + 20\n",
    "            \n",
    "            # Draw bounding box with emotion color\n",
    "            color = EMOTION_COLORS[emotion_label]\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, 3)\n",
    "            \n",
    "            # Display the emotion label and confidence\n",
    "            text = f\"{emotion_label}: {confidence:.1%}\"\n",
    "            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]\n",
    "            \n",
    "            # Background for text\n",
    "            cv2.rectangle(frame, (x_min, y_min - 40), \n",
    "                         (x_min + text_size[0] + 10, y_min), color, -1)\n",
    "            \n",
    "            # Text\n",
    "            cv2.putText(frame, text, (x_min + 5, y_min - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Draw face mesh (optional - can be disabled for better performance)\n",
    "            # mp_drawing.draw_landmarks(\n",
    "            #     frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            #     landmark_drawing_spec=None,\n",
    "            #     connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "            #         color=(0, 255, 0), thickness=1, circle_radius=1)\n",
    "            # )\n",
    "            \n",
    "            # Draw prediction bars\n",
    "            draw_prediction_bars(frame, [smoothed_prediction], emotion_label)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            cv2.putText(frame, \"Prediction Error\", (20, 40), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"No Face Detected\", (20, 40), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        # Clear prediction history when no face is detected\n",
    "        prediction_history.clear()\n",
    "\n",
    "    # Display FPS\n",
    "    cv2.putText(frame, f\"FPS: {fps:.1f}\", (W - 120, 30), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    # Display instructions\n",
    "    cv2.putText(frame, \"Press 'q' to quit, 'r' to reset\", (10, H - 20), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # Show the final frame\n",
    "    cv2.imshow('Facial Emotion Recognition (GNN)', frame)\n",
    "\n",
    "    # Handle key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('r'):\n",
    "        prediction_history.clear()\n",
    "        print(\"Prediction history reset\")\n",
    "    elif key == ord('s'):\n",
    "        filename = f\"emotion_frame_{int(time.time())}.jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(f\"Frame saved as {filename}\")\n",
    "\n",
    "print(\"Shutting down...\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "cap.release()\n",
    "cv2.waitKey(1) \n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "face_mesh.close()\n",
    "\n",
    "print(\"✅ Application closed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
